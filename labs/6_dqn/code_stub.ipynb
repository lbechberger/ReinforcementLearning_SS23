{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCLrRFHSKl_5"
   },
   "source": [
    "# Deep Q-Network with Lunar Lander\n",
    "\n",
    "This notebook shows an implementation of a DQN on the LunarLander environment.\n",
    "Details on the environment can be found [here](https://gym.openai.com/envs/LunarLander-v2/).\n",
    "\n",
    "Note: The following code is heavily inspired by [this]( https://www.katnoria.com/nb_dqn_lunar/) blog post.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2RNqaAGiLU1"
   },
   "source": [
    "## 1. Setup\n",
    "\n",
    "We first need to install some dependencies for using the environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install box2d pygame moviepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure to restart your kernel now!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CZXskDwXKl_-"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import sys\n",
    "from time import time\n",
    "from collections import deque, defaultdict, namedtuple\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tVO0INWR1DYS",
    "outputId": "68e453b9-79a7-4921-ac7c-186de15ec6c5"
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.reset(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lrq9VwzO1Zx4",
    "outputId": "ae7034e1-9628-4794-bba6-f7dac7ad5de4"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKg3BvSnivPE"
   },
   "source": [
    "## 2. Define the neural network, the replay buffer and the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9pG_Ii7jToR"
   },
   "source": [
    "First, we define the neural network that predicts the Q-values for all actions, given a state as input.\n",
    "This is a fully-connected neural net with two hidden layers using Relu activations.\n",
    "The last layer does not have any activation and outputs a Q-value for every action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JFxqeLkf1eHY"
   },
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)  \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q0xHaPTIj1pD"
   },
   "source": [
    "Next, we define a replay buffer that saves previous transitions (so-called `experiences`) and provides a `sample` function to randomly extract a batch of experiences from the buffer.\n",
    "\n",
    "Note that experiences are internally saved as `numpy`-arrays. They are converted back to PyTorch tensors before being returned by the `sample`-method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQw6QVAC1hQf"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        self.batch_size = batch_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.memory = deque(maxlen=buffer_size) # maximum size of buffer\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        experience = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(experience)\n",
    "                \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, self.batch_size)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        states = np.vstack([experience.state for experience in experiences if experience is not None])\n",
    "        states_tensor = torch.from_numpy(states).float().to(device)\n",
    "        \n",
    "        actions = np.vstack([experience.action for experience in experiences if experience is not None])\n",
    "        actions_tensor = torch.from_numpy(actions).long().to(device)\n",
    "\n",
    "        rewards = np.vstack([experience.reward for experience in experiences if experience is not None])\n",
    "        rewards_tensor = torch.from_numpy(rewards).float().to(device)\n",
    "\n",
    "        next_states = np.vstack([experience.next_state for experience in experiences if experience is not None])\n",
    "        next_states_tensor = torch.from_numpy(next_states).float().to(device)\n",
    "        \n",
    "        # Convert done flag from boolean to int\n",
    "        dones = np.vstack([experience.done for experience in experiences if experience is not None]).astype(np.uint8)\n",
    "        dones_tensor = torch.from_numpy(dones).float().to(device)\n",
    "        \n",
    "        return (states_tensor, actions_tensor, rewards_tensor, next_states_tensor, dones_tensor)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IYjlS7Fy1jJA"
   },
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # Replay memory size\n",
    "BATCH_SIZE = 64         # Number of experiences to sample from memory\n",
    "GAMMA = 0.99            # Discount factor\n",
    "TAU = 1e-3              # Soft update parameter for updating fixed q network\n",
    "LR = 1e-4               # Q Network learning rate\n",
    "UPDATE_EVERY = 4        # How often to update Q network\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "        # Initialize Q and Fixed Q networks\n",
    "        self.q_network = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.fixed_network = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters())\n",
    "        # Initiliase memory \n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "        self.timestep = 0\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        self.timestep += 1\n",
    "        \n",
    "        # trigger training\n",
    "        if self.timestep % UPDATE_EVERY == 0:\n",
    "            if len(self.memory) > BATCH_SIZE: # only when buffer is filled\n",
    "                sampled_experiences = self.memory.sample()\n",
    "                self.learn(sampled_experiences)\n",
    "        \n",
    "    def learn(self, experiences):\n",
    " \n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        action_values = self.fixed_network(next_states).detach()\n",
    "        max_action_values = action_values.max(1)[0].unsqueeze(1)\n",
    "\n",
    "        # If \"done\" just use reward, else update Q_target with discounted action values\n",
    "        Q_target = rewards + (GAMMA * max_action_values * (1 - dones))\n",
    "        Q_expected = self.q_network(states).gather(1, actions)\n",
    "\n",
    "        # Calculate loss and update weights\n",
    "        loss = F.mse_loss(Q_expected, Q_target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update fixed weights\n",
    "        self.update_fixed_network(self.q_network, self.fixed_network)\n",
    "        \n",
    "    def update_fixed_network(self, q_network, fixed_network):\n",
    "        for source_parameters, target_parameters in zip(q_network.parameters(), fixed_network.parameters()):\n",
    "            target_parameters.data.copy_(TAU * source_parameters.data + (1.0 - TAU) * target_parameters.data)\n",
    "        \n",
    "        \n",
    "    def act(self, state, eps=0.0):\n",
    "        rnd = random.random()\n",
    "        if rnd < eps:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "            action_values = self.q_network(state)\n",
    "            action = np.argmax(action_values.cpu().data.numpy())\n",
    "            return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2P3-UIm0fh3W"
   },
   "source": [
    "### 3. Executes episodes and train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NikAZhjNfsoi"
   },
   "source": [
    "We first define some paramters which are guiding the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lJGrZry81pu4"
   },
   "outputs": [],
   "source": [
    "MAX_EPISODES = 2000  # Max number of episodes to play\n",
    "MAX_STEPS = 1000     # Max steps allowed in a single episode/play\n",
    "\n",
    "# Epsilon schedule\n",
    "EPS_START = 1.0      # Default/starting value of eps\n",
    "EPS_DECAY = 0.999    # Epsilon decay rate\n",
    "EPS_MIN = 0.01       # Minimum epsilon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezOn9IpKf17C"
   },
   "source": [
    "Then we start executing episodes and observe the mean score per episode.\n",
    "The environment is considered as solved if this score is above 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_EC7XLJ1slY",
    "outputId": "ad110ade-36e5-4600-fcf0-faf2edc07235"
   },
   "outputs": [],
   "source": [
    "# Get state and action sizes\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print('State size: {}, action size: {}'.format(state_size, action_size))\n",
    "dqn_agent = DQNAgent(state_size, action_size, seed=0)\n",
    "start = time()\n",
    "\n",
    "# Maintain a list of last 100 scores\n",
    "scores_window = deque(maxlen=100)\n",
    "eps = EPS_START\n",
    "for episode in range(1, MAX_EPISODES + 1 ):\n",
    "    state, _ = env.reset()\n",
    "    score = 0\n",
    "    for t in range(MAX_STEPS):\n",
    "        action = dqn_agent.act(state, eps)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        dqn_agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state        \n",
    "        score += reward        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        eps = max(eps * EPS_DECAY, EPS_MIN)\n",
    "\n",
    "    scores_window.append(score)\n",
    "\n",
    "    if episode % 99 == 0:\n",
    "        mean_score = np.mean(scores_window)\n",
    "        print('Progress {}/{}, average score:{:.2f}'.format(episode, MAX_EPISODES, mean_score))\n",
    "\n",
    "    mean_score = np.mean(scores_window)\n",
    "    if mean_score >= 200:\n",
    "        print('\\rEnvironment solved in {} episodes, average score: {:.2f}'.format(episode, mean_score))\n",
    "        sys.stdout.flush()\n",
    "        break\n",
    "            \n",
    "end = time()    \n",
    "print('Took {} seconds'.format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd7QnYQRVUFc"
   },
   "source": [
    "### 4. Play epsiode and record it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gki1NW8sVlyd"
   },
   "source": [
    "Use the trained model to play and record one episode. The recorded video will be stored into the `video`-subfolder on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o-KWsd02TRZq",
    "outputId": "2b18059f-f9af-4ca7-ff69-cadfb97df3ac"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "FPS = 25 \n",
    "\n",
    "env = gym.make('LunarLander-v2', render_mode=\"rgb_array\")\n",
    "video = gym.wrappers.monitoring.video_recorder.VideoRecorder(env, \"video_lunarlander.mp4\")\n",
    "\n",
    "state, _ = env.reset()\n",
    "total_reward = 0.0\n",
    "\n",
    "while True:\n",
    "        start_ts = time.time()\n",
    "        env.render()\n",
    "        video.capture_frame()\n",
    "        \n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        action_values = dqn_agent.q_network(state)\n",
    "        action = np.argmax(action_values.cpu().data.numpy())\n",
    "\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        delta = 1/FPS - (time.time() - start_ts)\n",
    "        if delta > 0:\n",
    "            time.sleep(delta)\n",
    "\n",
    "print(\"Total reward: %.2f\" % total_reward)\n",
    "video.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "5_DQN_LunarLander.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
